{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b04b0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\14029\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np, json, re\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import json\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c0fde6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        con                                                txt\n",
      "0         0  Well it's great that he did something about th...\n",
      "1         0                       You are right Mr. President.\n",
      "2         0  You have given no input apart from saying I am...\n",
      "3         0  I get the frustration but the reason they want...\n",
      "4         0  I am far from an expert on TPP and I would ten...\n",
      "...     ...                                                ...\n",
      "949995    0  I genuinely can't understand how anyone can su...\n",
      "949996    0  As a reminder, this subreddit [is for civil di...\n",
      "949997    0                  K. Don't explain why or anything.\n",
      "949998    0                                          [deleted]\n",
      "949999    0  Ya, sociopaths are known for celebrating their...\n",
      "\n",
      "[950000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "##open file and apply to data fram\n",
    "file = 'controversial-comments.jsonl'\n",
    "\n",
    "data = []\n",
    "\n",
    "with open(file) as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        \n",
    "controversy = pd.DataFrame(data)\n",
    "\n",
    "print(controversy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea99b694",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "con  :  con\n",
      "txt  :  txt\n",
      "        con                                                txt\n",
      "0         0  well it's great that he did something about th...\n",
      "1         0                       you are right mr. president.\n",
      "2         0  you have given no input apart from saying i am...\n",
      "3         0  i get the frustration but the reason they want...\n",
      "4         0  i am far from an expert on tpp and i would ten...\n",
      "...     ...                                                ...\n",
      "949995    0  i genuinely can't understand how anyone can su...\n",
      "949996    0  as a reminder, this subreddit [is for civil di...\n",
      "949997    0                  k. don't explain why or anything.\n",
      "949998    0                                          [deleted]\n",
      "949999    0  ya, sociopaths are known for celebrating their...\n",
      "\n",
      "[950000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "##clean text\n",
    "stop = stopwords.words('english')\n",
    "import string\n",
    "def clean(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub('&lt;/?.*?&gt;',' &lt;&gt', text)\n",
    "    text=re.sub('\\\\d|\\\\W+',' ',text)\n",
    "    return text\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "\n",
    "for w in controversy:\n",
    "    print(w, \" : \", ps.stem(w))\n",
    "\n",
    "controversy = controversy.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "    \n",
    "print(controversy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8a630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert each text entry into a word-count vector (see sections 5.3 & 6.8 in the Machine Learning with Python Cookbook)\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "txtvec = cv.fit_transform(controversy['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdc608",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert each text entry into a part-of-speech tag vector (see section 6.7 in the Machine Learning with Python Cookbook).\n",
    "wpt = nltk.WordPunctTokenizer()\n",
    "text = wpt.tokenize('txt')\n",
    "text_tagged = nltk.pos_tag(text)\n",
    "new_text = []\n",
    "for word in text_tagged:\n",
    "    new_text.append(word[0] + \"/\" + word[1])\n",
    "\n",
    "doc = ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0792d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Convert each entry into a term frequency-inverse document frequency (tfidf) \n",
    "##vector (see section 6.9 in the Machine Learning with Python Cookbook).\n",
    "\n",
    "controversy = np.array('txt')\n",
    "tfidf = tfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd00b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "## when could each of the three different techniques be useful?\n",
    "\n",
    "## A - a word count vector could be used when figuring out which data can be cleaned and save the most room\n",
    "\n",
    "## B - The part of speech vector is useful so models dont have to learn grammatical structures from scratch\n",
    "\n",
    "## C - the TFIDF is useful in text mining applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
